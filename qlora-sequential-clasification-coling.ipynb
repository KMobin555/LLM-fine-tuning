{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:10:41.547486Z",
     "iopub.status.busy": "2024-10-13T17:10:41.547197Z",
     "iopub.status.idle": "2024-10-13T17:14:00.674122Z",
     "shell.execute_reply": "2024-10-13T17:14:00.672993Z",
     "shell.execute_reply.started": "2024-10-13T17:10:41.547442Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install Pytorch s\n",
    "%pip install \"torch==2.2.2\" tensorboard\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:14:00.677051Z",
     "iopub.status.busy": "2024-10-13T17:14:00.676613Z",
     "iopub.status.idle": "2024-10-13T17:14:23.859204Z",
     "shell.execute_reply": "2024-10-13T17:14:23.858285Z",
     "shell.execute_reply.started": "2024-10-13T17:14:00.677002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from huggingface_hub import login\n",
    "api_token = 'use your own token'\n",
    "login(api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:14:23.860925Z",
     "iopub.status.busy": "2024-10-13T17:14:23.860415Z",
     "iopub.status.idle": "2024-10-13T17:14:23.870257Z",
     "shell.execute_reply": "2024-10-13T17:14:23.869388Z",
     "shell.execute_reply.started": "2024-10-13T17:14:23.860890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def custom_sample_balanced_data(df, lang_column, label_column, lang_limits=None, random_state=42):\n",
    "    \n",
    "    # Initialize an empty DataFrame to store the final balanced data\n",
    "    balanced_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each unique language\n",
    "    for lang in df[lang_column].unique():\n",
    "        # Filter the DataFrame for the current language\n",
    "        lang_df = df[df[lang_column] == lang]\n",
    "        \n",
    "        # If the language is in the lang_limits dictionary, perform balanced sampling\n",
    "        if lang in lang_limits:\n",
    "            # Split by label to balance the labels (0 and 1)\n",
    "            label_0 = lang_df[lang_df[label_column] == 0]\n",
    "            label_1 = lang_df[lang_df[label_column] == 1]\n",
    "            \n",
    "            # Determine the maximum possible samples per label (50:50 ratio)\n",
    "            n_samples_per_label = min(len(label_0), len(label_1), lang_limits[lang] // 2)\n",
    "            \n",
    "            if n_samples_per_label > 0:\n",
    "                # Sample the rows for each label with equal number of rows\n",
    "                sampled_label_0 = label_0.sample(n=n_samples_per_label, random_state=random_state)\n",
    "                sampled_label_1 = label_1.sample(n=n_samples_per_label, random_state=random_state)\n",
    "                \n",
    "                # Combine the sampled data for the current language\n",
    "                balanced_lang_df = pd.concat([sampled_label_0, sampled_label_1], ignore_index=True)\n",
    "                \n",
    "                # Append to the final DataFrame\n",
    "                balanced_df = pd.concat([balanced_df, balanced_lang_df], ignore_index=True)\n",
    "        else:\n",
    "            # For languages not in lang_limits, keep all rows\n",
    "            balanced_df = pd.concat([balanced_df, lang_df], ignore_index=True)\n",
    "\n",
    "    # Shuffle the final DataFrame to mix the rows\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:14:23.874517Z",
     "iopub.status.busy": "2024-10-13T17:14:23.873773Z",
     "iopub.status.idle": "2024-10-13T17:14:24.032048Z",
     "shell.execute_reply": "2024-10-13T17:14:24.031219Z",
     "shell.execute_reply.started": "2024-10-13T17:14:23.874468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Define the system message for MGT detection\n",
    "# system_message = \"\"\"\n",
    "# You are a model trained to determine whether a given text is machine-generated or human-written. \n",
    "# Analyze the linguistic patterns, structure, and content of the text. \n",
    "# Based on your analysis, respond with either 'machine-generated' or 'human-written.'\n",
    "# \"\"\"\n",
    "\n",
    "# # Load Datasets\n",
    "# train_dataset = load_dataset('json', data_files='training_data.jsonl', split=\"train\")\n",
    "# valid_dataset = load_dataset('json', data_files='validation_data.jsonl', split=\"train\")\n",
    "\n",
    "\n",
    "# # Preprocess datasets for training\n",
    "# def format_mgt_data(examples):\n",
    "#     return (examples['text'],truncation=)\n",
    "\n",
    "# train_dataset_mapped = train_dataset.map(format_mgt_data, batched=True)\n",
    "# valid_dataset_mapped = valid_dataset.map(format_mgt_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:14:24.033531Z",
     "iopub.status.busy": "2024-10-13T17:14:24.033199Z",
     "iopub.status.idle": "2024-10-13T17:14:24.045861Z",
     "shell.execute_reply": "2024-10-13T17:14:24.045152Z",
     "shell.execute_reply.started": "2024-10-13T17:14:24.033500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_dataset_small[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:14:24.047228Z",
     "iopub.status.busy": "2024-10-13T17:14:24.046922Z",
     "iopub.status.idle": "2024-10-13T17:14:24.057858Z",
     "shell.execute_reply": "2024-10-13T17:14:24.056992Z",
     "shell.execute_reply.started": "2024-10-13T17:14:24.047197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:14:24.059742Z",
     "iopub.status.busy": "2024-10-13T17:14:24.059246Z",
     "iopub.status.idle": "2024-10-13T17:14:24.081223Z",
     "shell.execute_reply": "2024-10-13T17:14:24.080414Z",
     "shell.execute_reply.started": "2024-10-13T17:14:24.059699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig,\n",
    "    DataCollatorWithPadding, AutoTokenizer, set_seed, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True,\n",
    "                                  max_length=512\n",
    "                                 )\n",
    "\n",
    "def get_data(train_path, dev_path, test_path, random_seed):\n",
    "    \"\"\"\n",
    "    function to read dataframe with columns\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    val_df = pd.read_json(dev_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(f1_metric.compute(predictions=predictions, references = labels, average=\"macro\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "def fine_tune(train_df, valid_df, checkpoints_path, id2label, label2id, model_name, train_tem_args, class_weights):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True, # enable 4-bit quantization\n",
    "        bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "        bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r = 16, # the dimension of the low-rank matrices\n",
    "        lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "        bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "        task_type = 'SEQ_CLS'\n",
    "    )\n",
    "\n",
    "#     # Load a configuration with defaults (adjust parameters if necessary)\n",
    "#     config = AutoConfig.from_pretrained(\n",
    "#         model_name,\n",
    "#         rope_scaling={\n",
    "#             \"type\": \"linear\",  # or \"exponential\"\n",
    "#             \"factor\": 8.0\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        revision='main'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Model configurations for training\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # Ensure the model is on the correct device (GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "\n",
    "    # Tokenize datasets\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Calculate eval_steps to evaluate 3 times per epoch\n",
    "    total_train_samples = len(train_dataset)\n",
    "    batch_size = train_tem_args['train_batch']\n",
    "    steps_per_epoch = total_train_samples // batch_size\n",
    "    eval_steps = steps_per_epoch // 4  # Evaluate 3 times per epoch\n",
    "    eval_steps = 0.1\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=checkpoints_path,\n",
    "        learning_rate=train_tem_args['lr'],\n",
    "        per_device_train_batch_size=train_tem_args['train_batch'],\n",
    "        per_device_eval_batch_size=train_tem_args['val_batch'],\n",
    "        num_train_epochs=train_tem_args['epochs'],\n",
    "        weight_decay=train_tem_args['weight_decay'],\n",
    "        save_strategy=\"steps\",  # Save based on steps\n",
    "        logging_steps=eval_steps,  # Log every eval_steps\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_total_limit=2,  # Save only the last 2 checkpoints\n",
    "        save_steps=eval_steps,  # Save model every eval_steps\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5,  # Stop if no improvement after 3 evaluations\n",
    "        early_stopping_threshold=0.001  # Minimum improvement threshold\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping],  # Add early stopping callback\n",
    "#         class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # save best model\n",
    "    best_model_path = os.path.join(checkpoints_path, 'best')\n",
    "    \n",
    "    if not os.path.exists(best_model_path):\n",
    "        os.makedirs(best_model_path)\n",
    "    \n",
    "    trainer.save_model(best_model_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:19:58.233301Z",
     "iopub.status.busy": "2024-10-13T17:19:58.232863Z",
     "iopub.status.idle": "2024-10-13T17:20:13.381868Z",
     "shell.execute_reply": "2024-10-13T17:20:13.380895Z",
     "shell.execute_reply.started": "2024-10-13T17:19:58.233263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "\n",
    "# Set paths and parameters\n",
    "train_path = '/kaggle/input/coling-25-task-1/en_train.jsonl' \n",
    "dev_path = '/kaggle/input/coling-25-task-1/en_dev.jsonl'    \n",
    "test_path = '/kaggle/input/coling-25-task-1/en_devtest_text_id_only.jsonl'  \n",
    "checkpoints_path = '/kaggle/working/checkpoints'\n",
    "\n",
    "scoring_path = '/kaggle/input/short-coling/en_dev_for_score.jsonl'\n",
    "\n",
    "# train_path = '/kaggle/input/short-mul-lang-coling/multilingual_train_short.jsonl' \n",
    "# dev_path = '/kaggle/input/short-mul-lang-coling/multilingual_dev_short.jsonl' \n",
    "\n",
    "model = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "model  = \"meta-llama/Prompt-Guard-86M\"\n",
    "model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = \"/kaggle/input/llama-3/transformers/8b-hf/1\"\n",
    "mod3l = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "trained_model = \"llama_3_hf_custom\"\n",
    "\n",
    "prediction_path = '/kaggle/working/subtask_a_pred.jsonl'\n",
    "random_seed = 42\n",
    "\n",
    "# Set logging and seed\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "set_seed(random_seed)\n",
    "\n",
    "train_tem_args = {\n",
    "    'epochs': 3,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'train_batch': 4,\n",
    "    'val_batch': 16,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "id2label = {0: \"human\", 1: \"machine\"}\n",
    "label2id = {\"human\": 0, \"machine\": 1}\n",
    "\n",
    "\n",
    "#get data for train/dev/test sets\n",
    "train_df, valid_df, test_df = get_data(train_path, dev_path, test_path, random_seed)\n",
    "\n",
    "# lang_limits = {'en': 12000, 'zh': 9000}\n",
    "# # Call the function\n",
    "# train_df = custom_sample_balanced_data(train_df, lang_column='lang', label_column='label', lang_limits=lang_limits)\n",
    "\n",
    "# lang_limits = {'en': 9000, 'zh': 7000}\n",
    "# # Call the function\n",
    "# valid_df = custom_sample_balanced_data(valid_df, lang_column='lang', label_column='label', lang_limits=lang_limits)\n",
    "\n",
    "\n",
    "reduced_val = 1\n",
    "reduced_val = 0.005\n",
    "# Sample 10% of the DataFrame\n",
    "train_df = train_df.sample(n=int(len(train_df) * reduced_val), random_state=42)\n",
    "valid_df = valid_df.sample(n=int(len(valid_df) * reduced_val), random_state=42)\n",
    "\n",
    "print(train_df['lang'].value_counts(),valid_df['lang'].value_counts())\n",
    "\n",
    "\n",
    "print(\"data loaded--------------\")\n",
    "\n",
    "class_weights=(1/train_df.label.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:20:13.383841Z",
     "iopub.status.busy": "2024-10-13T17:20:13.383542Z",
     "iopub.status.idle": "2024-10-13T17:20:17.111332Z",
     "shell.execute_reply": "2024-10-13T17:20:17.110171Z",
     "shell.execute_reply.started": "2024-10-13T17:20:13.383811Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Initialize Weights & Biases (W&B) in disabled mode.\n",
    "import torch\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "!rm -r /kaggle/working/\n",
    "!rm -rf ~/.cache/huggingface\n",
    "!rm -rf ~/.cache/\n",
    "\n",
    "print(\"Training started----------------------------------------------\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "from huggingface_hub import login\n",
    "api_token = 'use your own'\n",
    "login(api_token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:20:17.113238Z",
     "iopub.status.busy": "2024-10-13T17:20:17.112895Z",
     "iopub.status.idle": "2024-10-13T17:22:32.339265Z",
     "shell.execute_reply": "2024-10-13T17:22:32.337210Z",
     "shell.execute_reply.started": "2024-10-13T17:20:17.113199Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train detector model\n",
    "print(\"Training Start --------------\")\n",
    "\n",
    "fine_tune(train_df, valid_df, trained_model, id2label, label2id, model, train_tem_args, class_weights)\n",
    "\n",
    "print(\"Training Done --------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.340295Z",
     "iopub.status.idle": "2024-10-13T17:22:32.340657Z",
     "shell.execute_reply": "2024-10-13T17:22:32.340502Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.340485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define your variables\n",
    "dir_to_zip = trained_model\n",
    "\n",
    "# Define the name of the output zip file\n",
    "last_word = model.split('/')[-1]\n",
    "output_zip = f\"eng_{trained_model}.zip\"\n",
    "\n",
    "# Create a zip file from the directory\n",
    "shutil.make_archive(output_zip.replace('.zip', ''), 'zip', dir_to_zip)\n",
    "\n",
    "print(f\"Zipped contents of {dir_to_zip} into {output_zip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.341839Z",
     "iopub.status.idle": "2024-10-13T17:22:32.342216Z",
     "shell.execute_reply": "2024-10-13T17:22:32.342043Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.342024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test(test_df, model_path, id2label, label2id):\n",
    "    \n",
    "    # load tokenizer from saved model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # load best model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    \n",
    "    # Ensure padding tokens are set for GPT-2\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "            \n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "#     metric = evaluate.load(\"bstrai/classification_report\")\n",
    "#     results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "    \n",
    "    # return dictionary of classification report\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.344315Z",
     "iopub.status.idle": "2024-10-13T17:22:32.344791Z",
     "shell.execute_reply": "2024-10-13T17:22:32.344568Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.344543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForSequenceClassification, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "def test(test_df, model_path, id2label, label2id, base_model_path):\n",
    "    # Configure the quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # 4-bit quantization\n",
    "        bnb_4bit_quant_type='nf4',  # optimal dtype for normally distributed weights\n",
    "        bnb_4bit_use_double_quant=True,  # double quantization\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16  # use bf16 for computation\n",
    "    )\n",
    "\n",
    "    # Load the base model with quantization config\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_path,  # Load the base model\n",
    "        quantization_config=quantization_config,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    # Load the LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=8,\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout=0.05,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS'\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Load the fine-tuned model weights from the checkpoint\n",
    "    model = PeftModel.from_pretrained(model, model_path)  # Use the checkpoint for fine-tuned weights\n",
    "    \n",
    "    # load adapter with the base model.\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    # Load the tokenizer from the base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "    # Ensure padding tokens are set for GPT-based models (optional)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Convert the test DataFrame to a Hugging Face Dataset\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    # Preprocess the test data\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Create the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Perform inference and get predictions\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "    # Return the predictions\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.346161Z",
     "iopub.status.idle": "2024-10-13T17:22:32.346644Z",
     "shell.execute_reply": "2024-10-13T17:22:32.346411Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.346385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "S\n",
    "\n",
    "\n",
    "# test detector model\n",
    "predictions = test(test_df, trained_model, id2label, label2id, model)\n",
    "\n",
    "# results, predictions = test(test_df, f\"testing\", id2label, label2id)\n",
    "\n",
    "\n",
    "# logging.info(results)\n",
    "predictions_df = pd.DataFrame({'id': test_df.id, 'label': predictions})\n",
    "predictions_df.to_json(prediction_path, lines=True, orient='records')\n",
    "\n",
    "print(\"Prediction Done --------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.348467Z",
     "iopub.status.idle": "2024-10-13T17:22:32.349196Z",
     "shell.execute_reply": "2024-10-13T17:22:32.348946Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.348919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_json(scoring_path, lines=True)\n",
    "\n",
    "test_df_tem = test_df[['id','text']]\n",
    "\n",
    "sampled_df = test_df_tem\n",
    "# Calculate 10% of the DataFrame\n",
    "# num_samples = int(len(test_df_tem) * 0.1)  # 10% of the total rows\n",
    "\n",
    "# Sample 10% of the DataFrame\n",
    "# sampled_df = test_df_tem.sample(n=num_samples, random_state=42)\n",
    "\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.350305Z",
     "iopub.status.idle": "2024-10-13T17:22:32.350775Z",
     "shell.execute_reply": "2024-10-13T17:22:32.350558Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.350533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicting for score --------------\")\n",
    "best_model_path = trained_model\n",
    "\n",
    "predictions = test(sampled_df, best_model_path, id2label, label2id)\n",
    "\n",
    "print(len(predictions),len(sampled_df))\n",
    "# logging.info(results)\n",
    "predictions_df = pd.DataFrame({'id': sampled_df.id, 'label': predictions})\n",
    "predictions_df.to_json(\"score_df.jsonl\", lines=True, orient='records')\n",
    "\n",
    "print(\"Predicting for score Done --------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.352198Z",
     "iopub.status.idle": "2024-10-13T17:22:32.352674Z",
     "shell.execute_reply": "2024-10-13T17:22:32.352441Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.352417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "COLUMNS = ['id', 'label']\n",
    "\n",
    "\n",
    "def check_format(file_path):\n",
    "  if not os.path.exists(file_path):\n",
    "    logging.error(\"File doesnt exists: {}\".format(file_path))\n",
    "    return False\n",
    "  \n",
    "  try:\n",
    "    submission = pd.read_json(file_path, lines=True)[['id', 'label']]\n",
    "  except:\n",
    "    logging.error(\"File is not a valid json file: {}\".format(file_path))\n",
    "    return False\n",
    "  \n",
    "  for column in COLUMNS:\n",
    "    if submission[column].isna().any():\n",
    "      logging.error(\"NA value in file {} in column {}\".format(file_path, column))\n",
    "      return False\n",
    "  \n",
    "  if not submission['label'].isin(range(0, 2)).all():\n",
    "    logging.error(\"Unknown Label in file {}\".format(file_path))\n",
    "    logging.error(\"Unique Labels in the file are {}\".format(submission['label'].unique()))\n",
    "    return False\n",
    "      \n",
    "  return True\n",
    "\n",
    "\n",
    "    \n",
    "pred_file_path = prediction_path \n",
    "  \n",
    "# for pred_file_path in prediction_file_path:\n",
    "check_result = check_format(pred_file_path)\n",
    "result = 'Format is correct' if check_result else 'Something wrong in file format'\n",
    "#     logging.info(\"Checking file: {}. Result: {}\".format(prediction_file_path, result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.354241Z",
     "iopub.status.idle": "2024-10-13T17:22:32.354773Z",
     "shell.execute_reply": "2024-10-13T17:22:32.354526Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.354482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging.handlers\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import sys\n",
    "# sys.path.append('.')\n",
    "# from format_checker import check_format\n",
    "\n",
    "\n",
    "def evaluate(pred_fpath, gold_fpath):\n",
    "  \n",
    "  pred_labels = pred_fpath\n",
    "  gold_labels = gold_fpath\n",
    "\n",
    "  print(gold_labels)\n",
    "  \n",
    "  merged_df = pred_labels.merge(gold_labels, on=['id'], suffixes=('_pred', '_gold'))\n",
    "\n",
    "  print(merged_df)\n",
    "\n",
    "  macro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"macro\", zero_division=0)\n",
    "  micro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"micro\", zero_division=0)\n",
    "  accuracy = accuracy_score(merged_df['label_gold'], merged_df['label_pred'])\n",
    "  \n",
    "  return macro_f1, micro_f1, accuracy\n",
    "\n",
    "\n",
    "def validate_files(pred_files):\n",
    "  if not check_format(pred_files):\n",
    "    logging.error('Bad format for pred file {}. Cannot score.'.format(pred_files))\n",
    "    return False\n",
    "  return True\n",
    "\n",
    "\n",
    "pred_file_path = predictions_df\n",
    "gold_file_path = test_df\n",
    "\n",
    "logging.info('Prediction file format is correct')\n",
    "macro_f1, micro_f1, accuracy = evaluate(pred_file_path, gold_file_path)\n",
    "logging.info(\"macro-F1={:.5f}\\tmicro-F1={:.5f}\\taccuracy={:.5f}\".format(macro_f1, micro_f1, accuracy))\n",
    "print(macro_f1, micro_f1, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-13T17:22:32.355922Z",
     "iopub.status.idle": "2024-10-13T17:22:32.356401Z",
     "shell.execute_reply": "2024-10-13T17:22:32.356184Z",
     "shell.execute_reply.started": "2024-10-13T17:22:32.356159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "macro_f1, micro_f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5700992,
     "sourceId": 9393827,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5765381,
     "sourceId": 9478835,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5785692,
     "sourceId": 9505903,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5852542,
     "sourceId": 9594632,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 39106,
     "modelInstanceId": 28079,
     "sourceId": 33547,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
